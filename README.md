# Shipping a Data Product â€“ Week 7 Challenge

## Overview & Motivation  
This project builds an end-to-end **ELT pipeline** that scrapes public Ethiopian medical Telegram channels, transforms the data, enriches it with computer vision, and serves analytics via an API. We aim to answer questions like â€œmost mentioned medical productsâ€ or â€œchannel posting trendsâ€ by collecting raw Telegram data into a **data lake**, loading it into a **PostgreSQL** warehouse, applying **dbt** transformations (star-schema modeling), detecting objects in images with **YOLOv8**, and exposing results through **FastAPI** endpoints. This modern stack ensures data is **reproducible, testable, and ready for analysis**, aligning with the challenge goals of delivering a robust data product.

## Tech Stack  
- **Python:** General-purpose programming language for scripting and services.  
- **Docker:** Container platform for packaging applications and dependencies into isolated, portable containers.  
- **PostgreSQL:** Advanced open-source object-relational database for warehousing our data.  
- **dbt (Data Build Tool):** SQL-based transformation framework (in-warehouse ELT), used for building staging tables and a dimensional star schema.  
- **FastAPI:** Modern, high-performance Python web framework for building RESTful APIs.  
- **YOLOv8 (Ultralytics):** State-of-the-art object detection model (â€œYou Only Look Onceâ€ v8) that is fast and accurate for image analysis.  
- **Dagster:** A modern data orchestrator for defining, scheduling, and monitoring complex pipelines.  

## Project Structure  
```
.
â”œâ”€â”€ .dbt/                # dbt configuration (profiles.yml, etc.)
â”œâ”€â”€ data/               
â”‚   â”œâ”€â”€ raw/             # Raw data lake (JSON outputs)
â”‚   â”‚   â”œâ”€â”€ telegram_messages/
â”‚   â”‚   â””â”€â”€ telegram_images/
â”‚   â””â”€â”€ clean/           # (For future cleaned data)
â”œâ”€â”€ notebooks/           # Jupyter notebooks for development tasks
â”‚   â”œâ”€â”€ task_1/scraping.ipynb
â”‚   â”œâ”€â”€ task_2/load_and_dbt.ipynb
â”‚   â””â”€â”€ task_3/yolo_enrich.ipynb
â”œâ”€â”€ models/              # dbt models (SQL files)
â”‚   â”œâ”€â”€ staging/         # Staging tables
â”‚   â””â”€â”€ marts/           # Dimensional models (facts, dims)
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ api/             # FastAPI application code
â”‚   â”œâ”€â”€ scrapper.py      # Telegram scraping script (Task 1)
â”‚   â”œâ”€â”€ raw_loader.py    # Loads raw JSON into Postgres (Task 2)
â”‚   â””â”€â”€ yolo_enrich.py   # YOLOv8 image processing (Task 3)
â”œâ”€â”€ Dockerfile           # Docker image for the application
â”œâ”€â”€ docker-compose.yml   # Compose for app + Postgres
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ .env                 # Environment variables (secrets, not in Git)
â””â”€â”€ README.md            # Project documentation (this file)
```

## Tasks Completed  

- **Task 0 â€“ Project Setup & Environment:**  
  - Initialized Git repo, created `requirements.txt`, `.env`, `Dockerfile`, and `docker-compose.yml`.  
- **Task 1 â€“ Data Scraping & Collection:**  
  - Scraped Telegram messages and images; stored raw JSON and images under `data/raw/`.  
- **Task 2 â€“ Loading & dbt Transformations:**  
  - Loaded raw data into PostgreSQL; built staging and mart models with dbt; added tests and documentation.  
- **Task 3 â€“ Image Enrichment with YOLOv8:**  
  - Ran YOLOv8 inference on scraped images; inserted detections into `analytics.fct_image_detections`.  

- **Task 4 â€“ Analytical API (FastAPI):**  
  - Build endpoints to answer business questions (top products, channel activity, posting trends).  
- **Task 5 â€“ Pipeline Orchestration (Dagster):**  
  - Orchestrate scraping, loading, transformations, and enrichment on a schedule via Dagster.  

## Setup & Usage  

1. **Clone & configure**:  
   ```bash
   git clone <repo-url>
   cd project-root
   cp .env.example .env
   # fill in your TELEGRAM_API_ID, etc.
   ```
2. **Docker (recommended)**:  
   ```bash
   docker-compose up -d
   ```
3. **Without Docker**:  
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```
4. **Run Scraper**:  
   ```bash
   python src/scrapper.py
   ```
5. **dbt**:  
   ```bash
   cd dbt_project
   dbt deps && dbt run && dbt test
   ```
6. **YOLO Enrichment**:  
   ```bash
   python src/yolo_enrich.py
   ```
7. **Start API**:  
   ```bash
   uvicorn src.api.main:app --reload --port 8000
   ```
8. **Dagster (future)**:  
   ```bash
   pip install dagster dagster-webserver
   dagster dev

## Tech Stack

- **Python**: Core scripting language for ETL, model inference, and API  
- **Telethon**: Telegram API client for scraping messages & media  
- **psycopg2**: PostgreSQL driver for schema setup and raw data loading  
- **SQLAlchemy**: ORM & session management for the FastAPI application  
- **PostgreSQL**: Data warehouse with `raw` and `analytics` schemas  
- **dbt (Data Build Tool)**: In-warehouse ELT for staging and dimensional (star-schema) modeling, testing, and documentation  
- **Ultralytics YOLOv8**: State-of-the-art object-detection model for image enrichment  
- **FastAPI**: High-performance Python web framework for REST endpoints  
- **Uvicorn**: ASGI server for serving the FastAPI app  
- **Dagster**: Pipeline orchestration, scheduling, and observability  
- **Docker & Docker Compose**: Containerization of services (API, dbt, Postgres, etc.)  
- **python-dotenv**: Secure management of environment variables via a `.env` file  
   ```

## Example API Requests  
```bash
curl "http://localhost:8000/api/reports/top-products?limit=5"
curl "http://localhost:8000/api/channels/lobelia4cosmetics/activity"
curl "http://localhost:8000/api/reports/posting-trends?interval=weekly"
```
--

## ğŸ“„ License

This project is open-source and available under the [Apache License](LICENSE).

## Next recommended Steps  
- Improve YOLO model accuracy with custom training data  
- Add authentication and user roles to the API  
- Deploy to a cloud environment with CI/CD