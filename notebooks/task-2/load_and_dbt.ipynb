{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f088c7b3",
   "metadata": {},
   "source": [
    "# Task 2 ‚Äì Data Modeling & Transformation\n",
    "\n",
    "\n",
    "## üìñ Overview\n",
    "\n",
    "In this task, we‚Äôll transform our raw JSON dumps into a clean, dimensional data warehouse in PostgreSQL using dbt. We‚Äôll build layered models‚Äîfrom raw loading scripts through staging tables to final star-schema marts‚Äîcomplete with testing and documentation\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "1. **Load raw JSON**  \n",
    "   - Write a Python script to bulk-load `data/raw/telegram_messages` JSON into a `raw.telegram_messages` schema in PostgreSQL.  \n",
    "2. **Initialize dbt**  \n",
    "   - Install dbt and the Postgres adapter.  \n",
    "   - Run `dbt init your_project_name` and configure `profiles.yml` to point at your warehouse.  \n",
    "3. **Develop Staging Models**  \n",
    "   - Create `models/staging/stg_telegram_messages.sql` to:  \n",
    "     - Cast JSON fields to proper types (e.g., timestamps).  \n",
    "     - Rename columns for clarity (e.g., `message_id`, `text`, `has_media`).  \n",
    "     - Extract nested JSON metadata (e.g., `media.file_id`, `media.file_size`).  \n",
    "4. **Build Data Marts (Star Schema)**  \n",
    "   - **Dimensions**  \n",
    "     - `models/marts/dim_channels.sql`: one row per channel with metadata (name, URL).  \n",
    "     - `models/marts/dim_dates.sql`: calendar table for time-based analysis.  \n",
    "   - **Fact**  \n",
    "     - `models/marts/fct_messages.sql`: one row per message, foreign keys to `dim_channels` and `dim_dates`, plus measures like `message_length`, `has_image`:contentReference[oaicite:1]{index=1}.  \n",
    "5. **Testing & Documentation**  \n",
    "   - Add dbt `tests` for:  \n",
    "     - Uniqueness of primary keys (`message_id`, `channel_id`).  \n",
    "     - Non-null critical fields (`text`, `timestamp`).  \n",
    "     - At least one custom test (e.g., no messages with empty text but `has_media = FALSE`).  \n",
    "   - Run `dbt docs generate` and review lineage & schema in the docs site.  \n",
    "6. **Run & Validate**  \n",
    "   - Execute `dbt run --models staging+ marts+`.  \n",
    "   - Use `dbt test` to ensure all tests pass.  \n",
    "   - Inspect the generated docs (`dbt docs serve`) to confirm model structure.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b1ccaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src path added: c:\\Users\\ABC\\Desktop\\10Acadamy\\week_7\\Shipping-a-Data-Product\\src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go two levels up from the notebook to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "\n",
    "# Join the path to 'src'\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Add 'src' to Python path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Confirm it's added\n",
    "print(\"src path added:\", src_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49aa461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Load .env and set PYTHONPATH\n",
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().cwd().parent / \"Shipping-a-Data-Product\"\n",
    "load_dotenv(dotenv_path=project_root / \".env\")\n",
    "sys.path.append(str(project_root / \"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfdf4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Run raw_loader to ingest JSON & images\n",
    "from raw_loader import load_messages, load_images\n",
    "\n",
    "# Load yesterday‚Äôs (or today‚Äôs) partitions\n",
    "load_messages(date_str=\"2025-07-13\")\n",
    "load_images(date_str=\"2025-07-13\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727d7563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DBT_PROFILES_DIR=./dbt_project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: dbt deps [OPTIONS]\n",
      "Try 'dbt deps -h' for help.\n",
      "\n",
      "Error: Got unexpected extra arguments (# fetches any packages)\n",
      "Usage: dbt seed [OPTIONS]\n",
      "Try 'dbt seed -h' for help.\n",
      "\n",
      "Error: Invalid value for '--profiles-dir': Path './dbt_project' does not exist.\n",
      "Usage: dbt run [OPTIONS]\n",
      "Try 'dbt run -h' for help.\n",
      "\n",
      "Error: Invalid value for '--profiles-dir': Path './dbt_project' does not exist.\n",
      "Usage: dbt test [OPTIONS]\n",
      "Try 'dbt test -h' for help.\n",
      "\n",
      "Error: Invalid value for '--profiles-dir': Path './dbt_project' does not exist.\n"
     ]
    }
   ],
   "source": [
    "# 4Ô∏è‚É£ Install & run dbt to build staging + marts\n",
    "%env DBT_PROFILES_DIR=./dbt_project\n",
    "!dbt deps   # fetches any packages\n",
    "!dbt seed   # if you have any seeds\n",
    "!dbt run    # builds models in analytics schema\n",
    "!dbt test   # runs built-in tests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
